import os
from flask import jsonify
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import datetime
import re # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì„í¬íŠ¸

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

# ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ë° ì„¤ì •
PERSIST_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../my_rag_db"))
COLLECTION_NAME = "admin_docs"

# í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ ì •ë³´
CURRENT_DATE = datetime.datetime.now().strftime("%Yë…„ %mì›” %dì¼")
CURRENT_TIME = datetime.datetime.now().strftime("%Hì‹œ %Më¶„")

# --- ê¸°ì¡´ ì½”ë“œì˜ 1, 2, 3ë²ˆ í•¨ìˆ˜ëŠ” LLM ê²½ìŸì„ ìœ„í•´ ì•½ê°„ ìˆ˜ì •ë˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤ ---

# âœ… 1. ë²¡í„° DB ë¡œë”© (ì„ë² ë”© ëª¨ë¸ì€ GoogleGenerativeAIEmbeddingsë¡œ í†µì¼)
def load_vectorstore():
    embedding = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=google_api_key
    )

    if not os.path.exists(PERSIST_DIR):
        raise ValueError("âŒ ë²¡í„° DB í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ ì£¼ì„¸ìš”.")


    vectordb = Chroma(
        persist_directory=PERSIST_DIR,
        collection_name=COLLECTION_NAME,
        embedding_function=embedding
    )
    return vectordb.as_retriever(search_kwargs={"k": 3})

# âœ… 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (í˜„ì¬ ë‚ ì§œ/ì‹œê°„ í”Œë ˆì´ìŠ¤í™€ë” ì¶”ê°€)
def get_subsidy_prompt():
    system_template = """ë„ˆëŠ” íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì˜ í›ˆë ¨ì¥ë ¤ê¸ˆ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì°¸ê³  ë¬¸ì„œ ë‚´ìš©ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´.
í˜„ì¬ ë‚ ì§œëŠ” {current_date} {current_time} ì…ë‹ˆë‹¤.

- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ìë£Œì— ì—†ìŒ"ì´ë¼ê³  ë§í•´.
- í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì¤˜.
- í•„ìš”í•œ ê²½ìš° bullet list í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ì¤˜.
- ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•´ë„ ì¢‹ì•„.

ì°¸ê³  ë¬¸ì„œ:
{context}
"""
    return ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("user", "{question}")
    ])

# âœ… 3. LCEL ì²´ì¸ êµ¬ì„± í•¨ìˆ˜ (LLM ëª¨ë¸ì„ ì¸ìë¡œ ë°›ì•„ ì²´ì¸ ìƒì„±)
def build_llm_chain(llm_model):
    retriever = load_vectorstore()
    prompt = get_subsidy_prompt()

    chain = (
        {
            "context": lambda x: "\n\n".join([doc.page_content for doc in retriever.get_relevant_documents(x["question"])]),
            "question": lambda x: x["question"],
            "current_date": lambda x: CURRENT_DATE, # í”„ë¡¬í”„íŠ¸ì— í˜„ì¬ ë‚ ì§œ ì „ë‹¬
            "current_time": lambda x: CURRENT_TIME  # í”„ë¡¬í”„íŠ¸ì— í˜„ì¬ ì‹œê°„ ì „ë‹¬
        }
        | prompt
        | llm_model
        | StrOutputParser()
    )
    return chain

# âœ… 4. ë‹µë³€ í‰ê°€ í•¨ìˆ˜ (ê°€ì¥ í•µì‹¬ì ì¸ ë¡œì§)
def evaluate_answers(question: str, answers: dict) -> str:
    """
    ì—¬ëŸ¬ LLMì˜ ë‹µë³€ì„ í‰ê°€í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì„ ì •í•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ê¸°ì¤€ì„ í†µí•´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³ , ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    best_answer = None
    best_score = -float('inf') # ì´ˆê¸° ì ìˆ˜ë¥¼ ë§¤ìš° ë‚®ê²Œ ì„¤ì •

    print("\n" + "="*50)
    print("âœ¨ ë‹µë³€ í‰ê°€ ì‹œì‘ âœ¨")
    print("="*50)

    # í‰ê°€ ê¸°ì¤€ì— ì‚¬ìš©ë  í‚¤ì›Œë“œ ë° íŒ¨í„´ ì •ì˜ (ì†Œë¬¸ì ë³€í™˜ í›„ ë¹„êµ)
    uncertainty_keywords = ["ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "ìë£Œì— ì—†ìŒ", "ì •ë³´ê°€ ì œí•œì ", "ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                            "íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤", "ì•Œë ¤ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤", "ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"]
    
    # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•, í•„ìš”ì‹œ KoNLPy ë“± NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥)
    question_keywords = [
        word for word in question.replace('?', '').replace('.', '').replace(',', '').split()
        if len(word) > 1 and word.lower() not in ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì™€", "ê³¼",
                                                 "ì–´ë–»ê²Œ", "ë˜ë‚˜ìš”", "ë¬´ì—‡ì¸ê°€ìš”", "ì–¸ì œ", "ì–´ë””ì„œ", "ë¬´ì—‡ì„", "ëˆ„ê°€", "ëŒ€í•œ", "ê´€í•œ"]
    ]
    # 'í›ˆë ¨ì¥ë ¤ê¸ˆ'ì€ í•­ìƒ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œì´ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
    if 'í›ˆë ¨ì¥ë ¤ê¸ˆ' not in [k.lower() for k in question_keywords]:
        question_keywords.append('í›ˆë ¨ì¥ë ¤ê¸ˆ')

    # ë‚ ì§œ/ì‹œê°„ ë° ìµœì‹  ì •ë³´ ê´€ë ¨ í‚¤ì›Œë“œ
    time_related_keywords = ["ìµœì‹ ", "í˜„ì¬", "ìµœê·¼", "ì˜¤ëŠ˜", "ì§€ê¸ˆ",
                              str(datetime.datetime.now().year), str(datetime.datetime.now().month)] # ì˜ˆ: 2024, 7 ë“±

    for model_name, answer_info in answers.items():
        answer_text = answer_info.get("answer", "")
        error = answer_info.get("error")
        score = 0
        answer_lower = answer_text.lower() # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ ìš©ì´

        print(f"\n--- í‰ê°€ ì¤‘: [{model_name}] ---")

        # 1. ì˜¤ë¥˜ê°€ ì—†ëŠ” ë‹µë³€ì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬ (ê¸°ë³¸ ì ìˆ˜ ë° í•„ìˆ˜ ì¡°ê±´)
        if error:
            score -= 500 # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ê±°ì˜ ì„ íƒë˜ì§€ ì•Šë„ë¡ ë§¤ìš° ë‚®ì€ ì ìˆ˜ ë¶€ì—¬
            continue # ì˜¤ë¥˜ê°€ ìˆëŠ” ë‹µë³€ì€ ë” ì´ìƒ í‰ê°€í•˜ì§€ ì•Šê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
        else:
            score += 100 # ì˜¤ë¥˜ ì—†ìŒì€ ê¸°ë³¸ ê°€ì 

        # 2. ë¶ˆí™•ì‹¤ì„±/ë¶€ì •í™•ì„± í‘œí˜„ ê°ì 
        for keyword in uncertainty_keywords:
            if keyword in answer_lower:
                score -= 70 # ë¶ˆí™•ì‹¤ì„± í‚¤ì›Œë“œ ë°œê²¬ ì‹œ í° ê°ì 
                break # í•˜ë‚˜ë§Œ ë°œê²¬ë˜ì–´ë„ í•´ë‹¹ ê°ì  ì ìš© í›„ ë‹¤ìŒ í‰ê°€ë¡œ
        
        # 3. í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ê°€ì 
        found_keywords_count = 0
        for keyword in question_keywords:
            if keyword.lower() in answer_lower:
                found_keywords_count += 1
        score += (found_keywords_count * 10) # í‚¤ì›Œë“œ 1ê°œë‹¹ 10ì  ê°€ì 

        # 4. ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ í¬í•¨ ì‹œ ê°€ì  (ê°€ë…ì„± í–¥ìƒ)
        if re.search(r'[-*â€¢]\s|^\d+\.\s', answer_text, re.MULTILINE): # ë¶ˆë¦¿ ë˜ëŠ” ìˆ«ì ëª©ë¡ íŒ¨í„´ í™•ì¸
            score += 20 # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë°œê²¬ ì‹œ ê°€ì 

        # 5. ë‹µë³€ ê¸¸ì´ì— ë”°ë¥¸ ì ìˆ˜ ë¶€ì—¬ (ì ì ˆí•œ ê¸¸ì´ ì„ í˜¸)
        answer_length = len(answer_text)
        if 50 < answer_length <= 300: # 50ì ì´ˆê³¼ 300ì ì´í•˜ì¼ ë•Œ ê°€ì  (í•µì‹¬ ì •ë³´ + ê°„ê²°ì„±)
            score += (answer_length // 20) # 20ì ë‹¹ 1ì  ê°€ì‚° (ìµœëŒ€ 15ì )
        elif answer_length <= 50: # ë„ˆë¬´ ì§§ì€ ë‹µë³€ ê°ì 
            score -= 20
        elif answer_length > 300: # ë„ˆë¬´ ê¸´ ë‹µë³€ ê°ì  (ìš”ì•½ ëŠ¥ë ¥ ë¶€ì¡±ìœ¼ë¡œ ê°„ì£¼, ê°ì  í­ì€ ì§§ì€ ê²ƒë³´ë‹¤ ì‘ê²Œ)
            score -= 10

        # 6. ë‚ ì§œ/ì‹œê°„ ë˜ëŠ” ìµœì‹  ì •ë³´ ì–¸ê¸‰ ì‹œ ê°€ì  (í”„ë¡¬í”„íŠ¸ì— í˜„ì¬ ë‚ ì§œë¥¼ ì£¼ë¯€ë¡œ, ì´ë¥¼ ë°˜ì˜í–ˆëŠ”ì§€ í™•ì¸)
        current_year = str(datetime.datetime.now().year)
        current_month = str(datetime.datetime.now().month)
        
        for keyword in time_related_keywords:
            if keyword.lower() in answer_lower or current_year in answer_text or current_month in answer_text:
                score += 15
                break # í•˜ë‚˜ë§Œ ë°œê²¬ë˜ì–´ë„ ê°€ì í•˜ê³  ë‹¤ìŒìœ¼ë¡œ

        # 7. êµ¬ì²´ì ì¸ ìˆ«ì/ë°ì´í„° í¬í•¨ ì‹œ ê°€ì  (ì˜ˆ: ê¸ˆì•¡, ê¸°ê°„ ë“±)
        if re.search(r'\d{1,3}(,\d{3})*(\.\d+)?', answer_text): # ìˆ«ì (ì²œë‹¨ìœ„ êµ¬ë¶„ì, ì†Œìˆ˜ì  í¬í•¨) íŒ¨í„´ í™•ì¸
            score += 15

        print(f"[{model_name}] ìµœì¢… ì ìˆ˜: {score}")

        # ìµœê³  ì ìˆ˜ ë‹µë³€ ê°±ì‹ 
        if score > best_score:
            best_score = score
            best_answer = answer_text
        elif score == best_score and best_answer is None: # ì²« ë²ˆì§¸ ë™ì ì¼ ê²½ìš°
            best_answer = answer_text
        # ë™ì ì¼ ê²½ìš° íŠ¹ì • ëª¨ë¸ ìš°ì„  ìˆœìœ„ë¥¼ ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì— ë¡œì§ ì¶”ê°€

    print("\n" + "="*50)
    print(f"ğŸ† ìµœì¢… ì„ ì • ë‹µë³€ ì ìˆ˜: {best_score}")
    print("="*50)

    if best_answer is None or best_answer.strip() == "":
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return best_answer

# âœ… 5. answer() í•¨ìˆ˜ (LLM ê²½ìŸ ë¡œì§ í¬í•¨)
def answer(question: str) -> str: # ë¹„ë™ê¸° ì œê±°
    if not question.strip():
        return jsonify({"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 200

    # ë‘ ê°€ì§€ Gemini LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm_gemini_flash = GoogleGenerativeAI(
        model="gemini-2.5-flash-lite", # ì²« ë²ˆì§¸ ê²½ìŸ ëª¨ë¸
        google_api_key=google_api_key,
        temperature=0.2,
        max_output_tokens=800
    )
    llm_gemini_pro = GoogleGenerativeAI(
        model="gemini-2.5-pro", # ë‘ ë²ˆì§¸ ê²½ìŸ ëª¨ë¸
        google_api_key=google_api_key,
        temperature=0.2,
        max_output_tokens=800
    )

    # ê° LLM ë³„ ì²´ì¸ ìƒì„±
    _chain_gemini_flash = build_llm_chain(llm_gemini_flash)
    _chain_gemini_pro = build_llm_chain(llm_gemini_pro)

    results = {}

    # Gemini Flash ë‹µë³€ ìƒì„±
    try:
        # invoke ë©”ì„œë“œëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
        flash_answer = _chain_gemini_flash.invoke({"question": question}) 
        results[llm_gemini_flash.model] = {"answer": flash_answer}
    except Exception as e:
        print(f"[âŒ {llm_gemini_flash.model} ì˜¤ë¥˜ ë°œìƒ]: {e}")
        results[llm_gemini_flash.model] = {"error": str(e)}

    # Gemini Pro ë‹µë³€ ìƒì„±
    try:
        # invoke ë©”ì„œë“œëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
        pro_answer = _chain_gemini_pro.invoke({"question": question})
        results[llm_gemini_pro.model] = {"answer": pro_answer}
    except Exception as e:
        print(f"[âŒ {llm_gemini_pro.model} ì˜¤ë¥˜ ë°œìƒ]: {e}")
        results[llm_gemini_pro.model] = {"error": str(e)}

    # ì·¨í•©ëœ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì  ë‹µë³€ ì„ íƒ ë° ë°˜í™˜
    response_text = evaluate_answers(question, results)
    return response_text;