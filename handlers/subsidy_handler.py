import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ë° ì„¤ì •
PERSIST_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../my_rag_db"))
COLLECTION_NAME = "admin_docs"

# âœ… 1. ë²¡í„° DB ë¡œë”©ë§Œ ìˆ˜í–‰ (ìƒì„± X)
def load_vectorstore():
    embedding = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

    if not os.path.exists(PERSIST_DIR):
        raise ValueError("âŒ ë²¡í„° DB í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ ì£¼ì„¸ìš”.")

    print("ğŸ“¦ ì €ì¥ëœ ë²¡í„° DB ë¡œë“œ ì¤‘...")

    vectordb = Chroma(
        persist_directory=PERSIST_DIR,
        collection_name=COLLECTION_NAME,
        embedding_function=embedding
    )

    return vectordb.as_retriever(search_kwargs={"k": 3})

# âœ… 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
def get_subsidy_prompt():
    system_template = """ë„ˆëŠ” íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì˜ í›ˆë ¨ì¥ë ¤ê¸ˆ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì°¸ê³  ë¬¸ì„œ ë‚´ìš©ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´.

- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ìë£Œì— ì—†ìŒ"ì´ë¼ê³  ë§í•´.
- í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì¤˜.
- í•„ìš”í•œ ê²½ìš° bullet list í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ì¤˜.
- ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•´ë„ ì¢‹ì•„.

ì°¸ê³  ë¬¸ì„œ:
{context}
"""
    return ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("user", "{question}")
    ])

# âœ… 3. LCEL ì²´ì¸ êµ¬ì„±
def build_chain():
    retriever = load_vectorstore()

    llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0.3,
        max_tokens=800,
        openai_api_key=OPENAI_API_KEY
    )

    prompt = get_subsidy_prompt()

    chain = (
        {
            "context": lambda x: "\n\n".join([doc.page_content for doc in retriever.get_relevant_documents(x["question"])]),
            "question": lambda x: x["question"]
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain

# âœ… 4. answer() í•¨ìˆ˜
_chain = build_chain()

def answer(question: str) -> str:
    if not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    try:
        return _chain.invoke({"question": question})
    except Exception as e:
        print(f"[âŒ ì˜¤ë¥˜ ë°œìƒ]: {e}")
        return "ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."