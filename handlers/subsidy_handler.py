import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 0. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ë²¡í„° DB ì €ì¥ ìœ„ì¹˜
PERSIST_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../rag/vectorstore"))
# Chroma ë‚´ ì»¬ë ‰ì…˜ ì´ë¦„
COLLECTION_NAME = "subsidy_docs"
# ì‹¤ì œ ì°¸ê³  .txt íŒŒì¼
SOURCE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../utils/subsidy_guide.txt"))

def ensure_vectorstore():
    embedding = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

    if not os.path.exists(PERSIST_DIR) or len(os.listdir(PERSIST_DIR)) == 0:
        print("ğŸ“¦ ë²¡í„° DBê°€ ì—†ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. ë¬¸ì„œ ë¡œë“œ
        loader = TextLoader(SOURCE_PATH, encoding="utf-8")
        documents = loader.load()

        # 2. ë¬¸ì„œ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_docs = splitter.split_documents(documents)

        # 3. ë²¡í„° DB ìƒì„±
        vectordb = Chroma.from_documents(
            documents=split_docs,
            embedding=embedding,
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION_NAME
        )
        vectordb.persist()
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ")
    else:
        print("ğŸ“¦ ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")

    # 4. DB ë¡œë”©
    vectordb = Chroma(
        persist_directory=PERSIST_DIR,
        collection_name=COLLECTION_NAME,
        embedding_function=embedding
    )
    return vectordb.as_retriever(search_kwargs={"k": 3})


# 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
def get_subsidy_prompt():
    system_template = """ë„ˆëŠ” íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì˜ í›ˆë ¨ì¥ë ¤ê¸ˆ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì°¸ê³  ë¬¸ì„œ ë‚´ìš©ë§Œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´.

- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ìë£Œì— ì—†ìŒ"ì´ë¼ê³  ë§í•´.
- í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì¤˜.
- í•„ìš”í•œ ê²½ìš° bullet list í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ì¤˜.
- ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•´ë„ ì¢‹ì•„.

ì°¸ê³  ë¬¸ì„œ:
{context}
"""
    return ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("user", "{question}")
    ])

#  6. LLM ì²´ì¸ êµ¬ì„± ë° ì‘ë‹µ ìƒì„±
def build_chain():
    retriever = ensure_vectorstore()

    llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0.3,
        max_tokens=800,
        openai_api_key=OPENAI_API_KEY
    )

    prompt = get_subsidy_prompt()

    # LCEL ì²´ì¸
    chain = (
        {
            "context": lambda x: "\n\n".join([doc.page_content for doc in retriever.get_relevant_documents(x["question"])]),
            "question": lambda x: x["question"]
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain

# ë‹µë³€ ìƒì„±
_chain = build_chain()

def answer(question: str) -> str:
    if not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    try:
        return _chain.invoke({"question": question})
    except Exception as e:
        print(f"[âŒ ì˜¤ë¥˜ ë°œìƒ]: {e}")
        return "ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
