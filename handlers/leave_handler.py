from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# ì„ë² ë”© ëª¨ë¸ (ê²€ìƒ‰ìš©)
embedding_model = OpenAIEmbeddings(
    openai_api_key=openai_key,
    model="text-embedding-3-small"
)

# ë²¡í„° DB ë¡œë”© -> /utils/vector_store_two.pyë¥¼ í†µí•´ ë²¡í„° DB ìµœì´ˆ ìƒì„±
VECTOR_DIR = "./utils/attendance_db"
COLLECTION_NAME = "leave_docs"

vectorstore = Chroma(
    collection_name=COLLECTION_NAME,
    persist_directory=VECTOR_DIR,
    embedding_function=embedding_model
)

# OpenAI GPT ëª¨ë¸
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.2,
    openai_api_key=openai_key
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    chain_type="stuff",
    return_source_documents=True
)

# ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  í•¨ìˆ˜
def answer(user_input: str) -> str:
    if not user_input:
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    try:
        result = qa_chain(user_input)

        # ë””ë²„ê¹…ìš©
        print("\nğŸ” [DEBUG] ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:", len(result["source_documents"]))
        for i, doc in enumerate(result["source_documents"]):
            print(f"\nğŸ“„ ë¬¸ì„œ {i+1}:\n{doc.page_content[:300]}")

        return str(result["result"])
    except Exception as e:
        print(f"[âŒ ì˜¤ë¥˜ ë°œìƒ]: {e}")
        return "ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
