from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# ì„ë² ë”© ëª¨ë¸
embedding_model = OpenAIEmbeddings(
    openai_api_key=openai_key,
    model="text-embedding-3-small"
)

# ë²¡í„° DB ê²½ë¡œì™€ ì»¬ë ‰ì…˜
VECTOR_DIR = "../my_rag_db"
COLLECTION_NAME = "chat_history"

# ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
vectorstore = Chroma(
    collection_name=COLLECTION_NAME,
    persist_directory=VECTOR_DIR,
    embedding_function=embedding_model
)

# ì²­í¬ ë¶„í• ê¸°
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

# ğŸŸ¢ ì‹¤ì‹œê°„ ëŒ€í™” ì €ì¥ í•¨ìˆ˜
def save_chat_to_vectorstore(user_input, bot_response, student_id="default"):
    chat = f"User: {user_input}\nBot: {bot_response}"
    doc = Document(page_content=chat, metadata={"source": "chat", "student_id": student_id})
    chunks = splitter.split_documents([doc])
    vectorstore.add_documents(chunks)

# ğŸ” ì¿¼ë¦¬ ìœ ì‚¬ ê²€ìƒ‰ í•¨ìˆ˜
def retrieve_context(query, k=3, student_id=None):
    if student_id:
        return vectorstore.similarity_search(query, k=k, filter={"student_id": student_id})
    return vectorstore.similarity_search(query, k=k)
