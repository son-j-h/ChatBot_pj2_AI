# íœ´ê°€/ì¡°í‡´/ë³‘ê°€/ê³µê°€ ê´€ë ¨í•´ì„œ ëŒ€ë‹µí•  ìˆ˜ ìˆê²Œë” ë²¡í„°DB íŒŒì¼ ìˆ˜ì •

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
embedding_model = OpenAIEmbeddings(
    openai_api_key=openai_key,
    model="text-embedding-3-small"
)

# ë²¡í„° DBê°€ ì €ì¥ë  í´ë” ìƒì„±
VECTOR_DIR = "./attendance_db"
COLLECTION_NAME = "leave_docs"

doc_paths = [
    "attendance_guide.txt"
]

# ë²¡í„° DBí™” ì‹œí‚¬ ë¬¸ì„œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
all_documents = []
for path in doc_paths:
    if not os.path.exists(path):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {path}")
        continue
    loader = TextLoader(path, encoding="utf-8")
    documents = loader.load()
    all_documents.extend(documents)

if not all_documents:
    raise ValueError("ğŸ“‚ ì„ë² ë”©í•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ì²­í¬í™”
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
split_docs = splitter.split_documents(all_documents)

vectorstore = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=VECTOR_DIR
)
vectorstore.persist()
print("âœ… ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ ë° Chroma ì €ì¥")