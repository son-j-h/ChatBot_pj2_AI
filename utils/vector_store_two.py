from langchain.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© (Google API í‚¤)
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    raise EnvironmentError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=google_api_key
)

# ë²¡í„° DB ì„¤ì •
VECTOR_DIR = "../my_rag_db"
COLLECTION_NAME = "admin_docs"

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
print("â–¶ BASE_DIR:", BASE_DIR)
print("â–¶ BASE_DIR contents:", os.listdir(BASE_DIR))

# íŒŒì¼ ê²½ë¡œ
target_file = os.path.join(BASE_DIR, "training_handbook.txt")
if not os.path.exists(target_file):
    raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: {target_file}")

# ì „ì²´ ë¬¸ì„œ ì½ê¸°
with open(target_file, encoding="utf-8") as f:
    full_text = f.read()

# ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ Document ê°ì²´ ìƒì„±
sections = full_text.split("\n\n")
documents = [
    Document(page_content=section.strip(), metadata={"source": target_file})
    for section in sections if section.strip()
]

if not documents:
    raise ValueError("ğŸ“‚ ë¬¸ì„œì—ì„œ ìœ íš¨í•œ ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ì²­í¬í™”: ë¬¸ë‹¨ ê¸°ì¤€ + í† í° ê¸°ì¤€
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
split_docs = splitter.split_documents(documents)

# ë²¡í„° DB ì €ì¥
vectorstore = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=VECTOR_DIR
)

vectorstore.persist()
print("âœ… ì „ì²´ ë¬¸ì„œ ì„ë² ë”© ë° Chroma ì €ì¥ ì™„ë£Œ")